{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a161076",
   "metadata": {},
   "source": [
    "## 1. Clone the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bbc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cyrilsofdevpro/SofAi.git\n",
    "!ls -la\n",
    "!ls -la SofAi\n",
    "!ls -la SofAi/backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52332148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Enable real model load and use Qwen2.5-0.5B-Instruct for quality Q&A (instruction-tuned, no loops)\n",
    "os.environ['LOAD_REAL_MODEL'] = '1'\n",
    "# Qwen2.5-0.5B-Instruct: small, instruction-tuned, free, coherent responses\n",
    "os.environ['MODEL_NAME'] = os.environ.get('MODEL_NAME', 'Qwen/Qwen2.5-0.5B-Instruct')\n",
    "\n",
    "display(Markdown(f\"**LOAD_REAL_MODEL**={os.environ['LOAD_REAL_MODEL']}, **MODEL_NAME**={os.environ['MODEL_NAME']}\"))\n",
    "\n",
    "# Confirm GPU availability\n",
    "print('Checking GPU (nvidia-smi):')\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf1b3a",
   "metadata": {},
   "source": [
    "## 2. Install dependencies (minimal set for fast startup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139af44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd SofAi/backend\n",
    "# Install only what we need for a quick demo (SKIP_MODEL_LOAD=1)\n",
    "!pip install --quiet fastapi uvicorn pyngrok flask-cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0440b",
   "metadata": {},
   "source": [
    "## 3. Start FastAPI backend (with dummy model for speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# If you want the notebook to load a real Hugging Face model (not dry-run),\n",
    "# set the notebook environment variable `LOAD_REAL_MODEL=1` before running this cell.\n",
    "# Example in a separate cell or runtime: import os; os.environ['LOAD_REAL_MODEL'] = '1'\n",
    "\n",
    "load_real = os.environ.get(\"LOAD_REAL_MODEL\", \"0\") in (\"1\", \"true\", \"True\")\n",
    "if load_real:\n",
    "    print(\"LOAD_REAL_MODEL=1 -> installing HF dependencies (this may take several minutes)...\")\n",
    "    # Install common HF deps. On Colab you should use a GPU runtime for larger models.\n",
    "    # Quiet install to reduce output; installation may still take several minutes.\n",
    "    get_ipython().system('pip install -q \"torch\" \"transformers[torch]\" accelerate peft safetensors')\n",
    "\n",
    "# Locate the backend folder (robust to different clone locations)\n",
    "search_paths = [Path(\"/content\"), Path(\"/root\"), Path('.')]\n",
    "backend_path = None\n",
    "for root in search_paths:\n",
    "    try:\n",
    "        for p in root.rglob(\"backend\"):\n",
    "            if (p / \"main.py\").exists():\n",
    "                backend_path = p\n",
    "                break\n",
    "        if backend_path:\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not backend_path:\n",
    "    # Fallback to common path used earlier\n",
    "    backend_path = Path('/content/SofAi/backend')\n",
    "\n",
    "print(f\"Using backend path: {backend_path}\")\n",
    "if not backend_path.exists():\n",
    "    raise FileNotFoundError(f\"backend path not found: {backend_path}. Did you clone the repo?\")\n",
    "\n",
    "os.chdir(str(backend_path))\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Files in current directory: {os.listdir('.')}\")\n",
    "\n",
    "# Set environment variable to control model loading in the server\n",
    "# SKIP_MODEL_LOAD=1 -> dry-run dummy model (fast)\n",
    "# SKIP_MODEL_LOAD=0 or unset -> load the real HF model (may be slow and require GPU)\n",
    "if load_real:\n",
    "    os.environ[\"SKIP_MODEL_LOAD\"] = \"0\"\n",
    "else:\n",
    "    os.environ[\"SKIP_MODEL_LOAD\"] = \"1\"\n",
    "\n",
    "# Kill any existing uvicorn process\n",
    "os.system(\"pkill -f 'uvicorn' 2>/dev/null || true\")\n",
    "# short pause\n",
    "time.sleep(1)\n",
    "\n",
    "# Start uvicorn in background: bind to 0.0.0.0 so ngrok can reach it\n",
    "print(\"Starting uvicorn backend...\")\n",
    "get_ipython().system_raw(\n",
    "    f'cd \"{backend_path}\" && python -m uvicorn main:app --host 0.0.0.0 --port 8000 --log-level info > uvicorn.log 2>&1 &'\n",
    ")\n",
    "\n",
    "# Wait for server to start and show startup logs\n",
    "time.sleep(5)\n",
    "print(\"\\n=== Uvicorn Startup Logs ===\")\n",
    "!tail -n 300 uvicorn.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173d2fd",
   "metadata": {},
   "source": [
    "## 4. Verify backend is listening locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"Checking if backend is listening...\")\n",
    "!curl -sS http://127.0.0.1:8000/health && echo \"\\n✓ /health endpoint working\" || echo \"✗ /health failed\"\n",
    "\n",
    "print(\"\\nTesting /chat endpoint...\")\n",
    "!curl -sS -X POST http://127.0.0.1:8000/chat \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"message\":\"hello from colab\",\"session_id\":\"test123\"}' && echo \"\\n✓ /chat endpoint working\" || echo \"✗ /chat failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69fb5f7",
   "metadata": {},
   "source": [
    "## 5. Start ngrok tunnel (public URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "from getpass import getpass\n",
    "\n",
    "# Option A: Use getpass to securely enter token (recommended)\n",
    "print(\"Enter your ngrok authtoken (from https://dashboard.ngrok.com/auth/your-authtoken):\")\n",
    "token = getpass(\"ngrok authtoken (will not echo): \")\n",
    "\n",
    "if token.strip():\n",
    "    ngrok.set_auth_token(token.strip())\n",
    "else:\n",
    "    print(\"⚠️ No token entered. Using default (may have connection limits).\")\n",
    "\n",
    "# Create tunnel: port 8000, TLS enabled for security\n",
    "print(\"Creating ngrok tunnel...\")\n",
    "tunnel = ngrok.connect(8000, bind_tls=True)\n",
    "PUBLIC_URL = tunnel.public_url\n",
    "\n",
    "print(f\"\\n✓ Public ngrok URL: {PUBLIC_URL}\")\n",
    "print(f\"\\n  Use this URL to access your backend from anywhere:\")\n",
    "print(f\"    Health check: {PUBLIC_URL}/health\")\n",
    "print(f\"    Chat endpoint: {PUBLIC_URL}/chat (POST)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc034b",
   "metadata": {},
   "source": [
    "## 6. Test public endpoint via ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import requests\n",
    "\n",
    "# Extract the public URL from the tunnel object\n",
    "from pyngrok import ngrok\n",
    "tunnels = ngrok.get_tunnels()\n",
    "public_url = tunnels[0].public_url if tunnels else None\n",
    "\n",
    "if public_url:\n",
    "    print(f\"Testing public endpoint: {public_url}\\n\")\n",
    "    \n",
    "    # Test /health\n",
    "    print(\"=== Public /health ===\")\n",
    "    try:\n",
    "        resp = requests.get(f\"{public_url}/health\", timeout=5)\n",
    "        print(resp.json())\n",
    "        print(\"✓ Public health check passed\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Public health check failed: {e}\\n\")\n",
    "    \n",
    "    # Test /chat POST\n",
    "    print(\"=== Public /chat POST ===\")\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            f\"{public_url}/chat\",\n",
    "            json={\"message\": \"test from public url\", \"session_id\": \"public_test\"},\n",
    "            timeout=5\n",
    "        )\n",
    "        print(resp.json())\n",
    "        print(\"✓ Public chat endpoint passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Public chat endpoint failed: {e}\")\n",
    "else:\n",
    "    print(\"✗ No ngrok tunnel found. Make sure to run the tunnel creation cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430c4be",
   "metadata": {},
   "source": [
    "## 7. (Optional) Check running processes and tunnel status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b059e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Running Processes ===\")\n",
    "!ps aux | grep -E \"uvicorn|python\" | grep -v grep\n",
    "\n",
    "print(\"\\n=== ngrok Tunnel Status ===\")\n",
    "!curl -s http://127.0.0.1:4040/api/tunnels | python -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6f0a8",
   "metadata": {},
   "source": [
    "## 8. Frontend Setup (if using local React frontend)\n",
    "\n",
    "If you want to test the React UI locally with this Colab backend:\n",
    "\n",
    "1. In your local terminal, from `SofAI/frontend`:\n",
    "   ```bash\n",
    "   export VITE_API_BASE=https://<your-ngrok-url>  # use the PUBLIC_URL from step 5\n",
    "   npm run dev\n",
    "   ```\n",
    "\n",
    "2. Open http://localhost:3001 in your browser and send messages.\n",
    "\n",
    "The frontend will call your public ngrok endpoint. CORS is enabled in the FastAPI backend, so cross-origin requests should work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96010520",
   "metadata": {},
   "source": [
    "## 9. Debugging: View backend logs if something fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show latest uvicorn logs (useful if endpoint tests failed)\n",
    "print(\"=== Latest Uvicorn Logs ===\")\n",
    "!tail -n 500 uvicorn.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc40ba8",
   "metadata": {},
   "source": [
    "## 10. Stop backend (when done)\n",
    "\n",
    "Run this cell to cleanly stop uvicorn and ngrok tunnels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ca172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyngrok import ngrok\n",
    "\n",
    "print(\"Stopping uvicorn...\")\n",
    "os.system(\"pkill -f 'uvicorn' 2>/dev/null || true\")\n",
    "\n",
    "print(\"Closing ngrok tunnels...\")\n",
    "ngrok.kill()\n",
    "\n",
    "print(\"✓ Backend and ngrok stopped.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
